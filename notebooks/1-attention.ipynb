{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-Attention Mechanism: From Scratch\n",
        "\n",
        "This notebook implements the self-attention mechanism from scratch using PyTorch.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Mathematical Foundation](#math)\n",
        "3. [PyTorch Implementation](#pytorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction {#introduction}\n",
        "\n",
        "### How we represent the attention\n",
        "\n",
        "Attention should be calculated from an input -> target processed with a weight. Take an example, if we concat user embedding and an item embedding, and map to how a user could relate to an item. The output we are looking for will be the attention for user -> item.\n",
        "\n",
        "### Now, let's talk about self-attention\n",
        "\n",
        "Self attention is a unique condition, where the input and target is the same object.\n",
        "\n",
        "Take a look at following formula:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Let's understand this step by step:\n",
        "\n",
        "1. $QK^T$ will calculate the heatmap of how x focuses on itself:\n",
        "   ```\n",
        "   X    a    b    c\n",
        "   a    2    1    4\n",
        "   b    3    3    1\n",
        "   c    8    1    2\n",
        "   ```\n",
        "\n",
        "2. Softmax will apply a normalization for this table, and make sum of all elements into 1\n",
        "\n",
        "3. For each attention after softmax, we process another dot product, as we discussed above, dot product can represent internal relationship between two things.\n",
        "\n",
        "4. After all these processes, we are going to have an attention tensor, the higher score we have, it means the higher relation they have.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Query (Q)**: What am I looking for?\n",
        "- **Key (K)**: What do I have to offer?\n",
        "- **Value (V)**: What is the actual content?\n",
        "\n",
        "In self-attention, Q, K, and V are all derived from the same input sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Foundation {#math}\n",
        "\n",
        "The self-attention mechanism can be expressed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- $Q$: Query matrix (n × d_k)\n",
        "- $K$: Key matrix (n × d_k) \n",
        "- $V$: Value matrix (n × d_v)\n",
        "- $d_k$: Dimension of key vectors\n",
        "- $\\sqrt{d_k}$: Scaling factor to prevent vanishing gradients\n",
        "\n",
        "### Step-by-step process:\n",
        "1. Compute attention scores: $QK^T$\n",
        "2. Scale by $\\sqrt{d_k}$\n",
        "3. Apply softmax to get attention weights\n",
        "4. Multiply by values: $\\text{softmax}(\\cdot)V$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch Implementation {#pytorch}\n",
        "\n",
        "Let's implement self-attention using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention mechanism implementation in PyTorch.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_k: int = None, d_v: int = None):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k if d_k is not None else d_model\n",
        "        self.d_v = d_v if d_v is not None else d_model\n",
        "        \n",
        "        # Linear transformations for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, self.d_k, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, self.d_k, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, self.d_v, bias=False)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(self.d_v, d_model, bias=False)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of self-attention.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, d_model)\n",
        "            \n",
        "        Returns:\n",
        "            output: Attention output (batch_size, seq_len, d_model)\n",
        "            attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = self.W_q(x)  # (batch_size, seq_len, d_k)\n",
        "        K = self.W_k(x)  # (batch_size, seq_len, d_k)\n",
        "        V = self.W_v(x)  # (batch_size, seq_len, d_v)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, seq_len, seq_len)\n",
        "        scores = scores / np.sqrt(self.d_k)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        attended_values = torch.matmul(attention_weights, V)  # (batch_size, seq_len, d_v)\n",
        "        \n",
        "        # Output projection\n",
        "        output = self.W_o(attended_values)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the implementation\n",
        "d_model = 8\n",
        "seq_len = 4\n",
        "batch_size = 1\n",
        "\n",
        "# Create sample input\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Initialize attention layer\n",
        "attention = SelfAttention(d_model)\n",
        "\n",
        "# Forward pass\n",
        "output, attention_weights = attention(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"\\nAttention weights (first sample):\")\n",
        "print(attention_weights[0].detach().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a longer sequence\n",
        "seq_len = 8\n",
        "x_long = torch.randn(batch_size, seq_len, d_model)\n",
        "output_long, attention_weights_long = attention(x_long)\n",
        "\n",
        "print(f\"Longer sequence attention weights shape: {attention_weights_long.shape}\")\n",
        "print(f\"Attention weights (first sample):\")\n",
        "print(attention_weights_long[0].detach().numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we implemented the self-attention mechanism from scratch using PyTorch:\n",
        "\n",
        "1. **Mathematical Foundation**: Understanding the attention formula and scaling\n",
        "2. **PyTorch Implementation**: Complete self-attention layer with proper linear transformations\n",
        "\n",
        "### Key Takeaways:\n",
        "- Self-attention allows models to focus on relevant parts of the input\n",
        "- The mechanism is highly parallelizable and interpretable\n",
        "- Attention weights reveal what the model is \"looking at\"\n",
        "- The scaling factor $\\sqrt{d_k}$ prevents vanishing gradients\n",
        "\n",
        "This foundation is essential for understanding more complex architectures like Transformers!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
